# Shadowing365: Intelligent Shadowing Video Generator
# Êô∫ËÉΩÂΩ±Â≠êË∑üËØªËßÜÈ¢ëÁîüÊàêÂô®

Does your language learning feel dry? Shadowing365 turns any PDF book into an engaging, "high-frequency" shadowing video. With Solarized visuals, karaoke-style highlighting, and AI-powered insights, it transforms reading into an immersive audio-visual experience.

‰Ω†ÁöÑËØ≠Ë®ÄÂ≠¶‰π†ÊòØÂê¶ÊûØÁá•‰πèÂë≥ÔºüShadowing365 Â∞Ü‰ªª‰Ωï PDF ‰π¶Á±çËΩ¨Âåñ‰∏∫Âºï‰∫∫ÂÖ•ËÉúÁöÑ‚ÄúÈ´òÈ¢ë‚ÄùÂΩ±Â≠êË∑üËØªËßÜÈ¢ë„ÄÇÈÖçÂêà Solarized Êä§ÁúºËßÜËßâ„ÄÅÂç°ÊãâOKÂºèÁöÑÈ´ò‰∫ÆÊåáËØª‰ª•Âèä AI ÁîüÊàêÁöÑË∂£Âë≥Áü•ËØÜÁÇπÔºåËÆ©ÈòÖËØªÂèòÊàêÊ≤âÊµ∏ÂºèÁöÑËßÜÂê¨‰ΩìÈ™å„ÄÇ

---

## üéØ Purpose & Design Philosophy (ËÆæËÆ°ÂàùË°∑)
The core goal is **Deep Immersion (Ê∑±Â∫¶Ê≤âÊµ∏)** and **Eye Comfort (Áî®ÁúºËàíÈÄÇ)**.

*   **Visual Ergonomics**: We use the **Solarized Light** theme (Cream background + Dark Blue/Grey text) to minimize eye strain during long practice sessions.
*   **Shadowing Optimization**:
    *   **Karaoke Highlighting**: Words light up as they are spoken, guiding your rhythm and focus.
    *   **Natural Pacing**: TTS is slowed by 10% to match a comfortable shadowing speed.
    *   **Inspiration Cards (ÁÅµÊÑüÂç°Áâá)**: A stylish card displays English idioms, slang, or cultural nuggets found in the text, explained in Chinese. Help you learn English in depth.

Ê†∏ÂøÉÁõÆÊ†áÊòØÂÆûÁé∞ **Ê∑±Â∫¶Ê≤âÊµ∏** Âíå **Áî®ÁúºËàíÈÄÇ**„ÄÇ
*   **ËßÜËßâ‰∫∫‰ΩìÂ∑•Â≠¶**: ÈááÁî® **Solarized Light** ‰∏ªÈ¢ò (Â•∂ÈÖ™Ëâ≤ËÉåÊôØ + Ê∑±ÁÅ∞ËìùÊñáÂ≠ó)ÔºåÊúÄÂ§ßÁ®ãÂ∫¶ÂáèÂ∞ëÈïøÊó∂Èó¥ÁªÉ‰π†ÁöÑÁúºÈÉ®Áñ≤Âä≥„ÄÇ
*   **Ë∑üËØª‰ºòÂåñ**:
    *   **Âç°ÊãâOKÈ´ò‰∫Æ**: ÂçïËØçÈöèËØ≠Èü≥ÈÄê‰∏™ÁÇπ‰∫ÆÔºåÂºïÂØº‰Ω†ÁöÑËäÇÂ•èÂíåÊ≥®ÊÑèÂäõ„ÄÇ
    *   **Ëá™ÁÑ∂ËØ≠ÈÄü**: ËØ≠Èü≥ÈÄüÂ∫¶Èôç‰Ωé 10%ÔºåÂåπÈÖçËàíÈÄÇÁöÑË∑üËØªËäÇÂ•è„ÄÇ
    *   **ÁÅµÊÑüÂç°Áâá**: Â±èÂπïÂ∫ïÈÉ®‰ºöÂá∫Áé∞Á≤æÂøÉËÆæËÆ°ÁöÑ‚ÄúÁü•ËØÜÂç°Áâá‚ÄùÔºåÊ∑±Â∫¶Ëß£ÊûêÂè•‰∏≠ÁöÑËã±ËØ≠Âú∞ÈÅìË°®ËææÔºà‰øöËØ≠„ÄÅÊñáÂåñÊ¢óÔºâÔºåÊåñÊéòËØ≠Ë®ÄËÉåÂêéÁöÑË∂£Âë≥„ÄÇ

---

## üõ† Features (ÂäüËÉΩÁâπÊÄß)
*   **PDF to Video**: Extracts text from PDF, segments it into sentences.
*   **AI Translation**: Integrates **Ollama** (offline) or online APIs for context-aware translation.
*   **Bilingual Display**: Source (English/German) + Target (Chinese) subtitles.
*   **Karaoke Logic**: Pseudo-alignment algorithm estimates word timing for dynamic highlighting.
*   **Inspiration Cards**: Auto-generates insights based on the **Inspiration Prompt**:

> **Inspiration Card Prompt (ÁÅµÊÑüÂç°Áâá Prompt)**:
> ```
> Analyze this English sentence for learners: '[SOURCE]'. 
> Find something TRULY fascinating or a common usage trap. Share: 
> - A core mental model (how native speakers 'see' this word), OR 
> - A vivid metaphor/idiom, OR 
> - A structural pattern that makes them sound smart. 
> Avoid generic facts. Be inspired and natural. 
> NEVER say 'Êó†Âõ∫ÂÆö‰π†ËØ≠'. If nothing obvious, connect it to a related concept. 
> Max 25 words in Chinese. Output ONLY the insight.
> ```

---

## üíª Technical Stack (ÊäÄÊúØÊ†à)
*   **Language**: Python 3.11+
*   **Core Libraries**:
    *   `PyMuPDF` (Extraction)
    *   `spaCy` (NLP/Segmentation)
    *   `MoviePy` & `Pillow` (Video Synthesis)
    *   `Ollama` (Local LLM Integration)
    *   `Edge-TTS` (Natural Speech Synthesis)

---

## ‚öôÔ∏è Hardware & Model Recommendations (Á°¨‰ª∂‰∏éÊ®°ÂûãÊé®Ëçê)

### GPU Requirements (GPU Ë¶ÅÊ±Ç)
*   **Lightweight**: CPU-only is possible but slow for translation.
*   **Recommended**: Nvidia GPU (8GB+ VRAM).
*   **User Setup (A5500 / 24GB VRAM)**:
    *   You have an **Nvidia RTX A5500 (24GB)**. This is powerful but has limits.

### AI Model Strategy (AI Ê®°ÂûãÁ≠ñÁï•)
To get the best translation results:

1.  **DeepSeek-V3 (The "Best" So Far)**:
    *   **Offline**: Impossible on single 24GB card (Requires ~350GB+ VRAM).
    *   **Solution**: Use the **DeepSeek API** (Online). It is extremely cheap and effective.
    *   **Config**: Set `translation_provider: openai` and use DeepSeek base URL.

2.  **Qwen-2.5-14B (Best Offline Option)**:
    *   Fits comfortably in 24GB VRAM.
    *   Excellent bilingual capability.
    *   Fast inference.

3.  **Qwen-2.5-32B-Int4**:
    *   Fits tightly in 20-22GB VRAM.
    *   Better reasoning than 14B.

---

## üöÄ Usage (‰ΩøÁî®ÊñπÊ≥ï)

### 1. Installation 
```bash
pip install -r requirements.txt
# Ensure you have 'ollama' installed system-wide
```

### 2. Configuration (`config.yaml`)
Customize your experience:
```yaml
input_file: "input/my_book.pdf"
source_lang: "en"
translation_provider: "ollama"  # or 'openai' for DeepSeek API
ollama_model: "qwen2.5:14b"
enable_trivia: true # Turn on/off inspiration text
theme: "solarized_light"
```

### 3. Run
```bash
python pdf_to_video.py
```
Check `output/` for your video!

---

## üîÆ Roadmap (Êú™Êù•Â±ïÊúõ)
*   **Scrolling Subtitles (Teleprompter Mode)**: Continuous scrolling text for fluid reading.
*   **Strict Alignment**: Integrate `Aeneas` or `Montreal Forced Aligner` for millisecond-perfect karaoke.
*   **Multi-Speaker**: Assign different voices to different characters in fiction books.
*   **Mobile App**: Package as a Flutter/React Native app for on-the-go generation.

---

## üêü Fish Speech Integration (SOTA Voice Cloning)

We have integrated **Fish Speech 1.5** (SOTA Open Source TTS) for cinema-grade voice quality.

### Pros & Cons (‰ºòÁº∫ÁÇπ)
*   **‚úÖ Pros**: 
    *   **Incredible Realism**: Far superior to standard TTS. Sounds like a real human breathing and pausing.
    *   **Voice Cloning**: Clone *any* voice from a 15s reference audio (British, American, Anime characters, etc.).
    *   **Context Aware**: Understands emotion and prosody better.
*   **‚ùå Cons**:
    *   **Heavy Resource Usage**: Requires Nvidia GPU (8GB+ VRAM recommended).
    *   **Slower Generation**: Unlike Edge-TTS (instant), Fish Speech takes time (~2-5s per sentence on A5500).
    *   **Complex Deployment**: Requires a dedicated Docker API server.

### üõ°Ô∏è Reliability & Deploying (ÈÉ®ÁΩ≤ÈöæÁÇπ‰∏éÊñπÊ°à)
Fish Speech is notoriously hard to deploy due to dependency conflicts. We solved this by:
1.  **Docker Isolation**: Running the API server in a validated container (`fishaudio/fish-speech:latest-server-cuda`).
2.  **Critical Server Patches**: We successfully patched the server core to fix:
    *   **Crash on WAV loading**: Switched from `ffmpeg` to `soundfile` backend.
    *   **415 Errors**: Implemented manual Base64 decoding for robust API communication.
3.  **Resume Capability (Êñ≠ÁÇπÁª≠‰º†)**: 
    *   **Smart Checkpointing**: The script now saves `step1_translated.json` and `step2_audio.json`.
    *   **Crash Protection**: If the 5-hour task crashes at 99%, simply restart. It will **Skip** already translated text and **Skip** already generated audio files, finishing the rest in seconds.
    *   **Visual Progress**: Added `tqdm` progress bars to tell you exactly how long is left.

### Quick Start with Fish Speech
1.  **Start Server**: `docker compose -f fish_speech/docker-compose.yml up -d`
2.  **Config**: Set `tts_provider: "fish_speech"` in `config.yaml`.
3.  **Choose Your Voice**: We have included several high-quality samples in `input/`:
    *   `ref_voice_obama.wav`: Iconic rhetorical American (Barack Obama).
    *   `ref_voice_pure_american.wav`: Clean, patient narration (Phil Chenevert).
    *   `ref_voice_us_broadcast.wav`: Standard broadcast style.
    *   `ref_voice.wav`: Professional British accent.
4.  **Legal Attribution**: Please refer to [CREDITS.md](CREDITS.md) for licensing and source information for these voice samples.
5.  **Run**: `python pdf_to_video.py`
# shadowing365
